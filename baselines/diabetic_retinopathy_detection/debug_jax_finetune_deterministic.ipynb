{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf43a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dm-haiku in /home/nband/ub_venv/lib/python3.8/site-packages (0.0.5)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/nband/ub_venv/lib/python3.8/site-packages (from dm-haiku) (0.8.9)\r\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /home/nband/ub_venv/lib/python3.8/site-packages (from dm-haiku) (0.12.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/nband/ub_venv/lib/python3.8/site-packages (from dm-haiku) (1.20.3)\r\n",
      "Requirement already satisfied: jmp>=0.0.2 in /home/nband/ub_venv/lib/python3.8/site-packages (from dm-haiku) (0.0.2)\r\n",
      "Requirement already satisfied: six in /home/nband/ub_venv/lib/python3.8/site-packages (from absl-py>=0.7.1->dm-haiku) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "# ! pip install wandb\n",
    "! pip install dm-haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d2ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 03:11:19.682904: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-22 03:11:19.682940: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 03:11:21.881897: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-22 03:11:21.881945: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-22 03:11:21.881967: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (t1v-n-9dd13ff2-w-0): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "tf.config.experimental.set_visible_devices([], 'TPU_SYSTEM')\n",
    "tf.config.experimental.set_visible_devices([], 'TPU')\n",
    "\n",
    "print(tf.config.experimental.get_visible_devices())\n",
    "\n",
    "import uncertainty_baselines as ub\n",
    "\n",
    "import wandb\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import flax.jax_utils as flax_utils\n",
    "from functools import partial  # pylint: disable=g-importing-member so standard\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import numbers\n",
    "import os\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from clu import metric_writers\n",
    "from clu import parameter_overview\n",
    "from clu import periodic_actions\n",
    "from clu import preprocess_spec\n",
    "import flax\n",
    "import flax.jax_utils as flax_utils\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import robustness_metrics as rm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.io import gfile\n",
    "import checkpoint_utils  # local file import\n",
    "import input_utils  # local file import\n",
    "import train_utils  # local file import\n",
    "import preprocess_utils  # local file import\n",
    "# local file import\n",
    "from imagenet21k_vit_base16_finetune_country_shift import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb17235",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'tmp/diabetic_retinopathy_detection/vit-16-i21k'\n",
    "tf.io.gfile.makedirs(output_dir)\n",
    "logging.info('Saving checkpoints at %s', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1a6371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "from vit16_deterministic import accumulate_gradient_with_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa37e7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jax local devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
     ]
    }
   ],
   "source": [
    "print('Number of Jax local devices:', jax.local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505f5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa9d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d0aa600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 03:11:38.156776: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/nband/ub_venv/lib/python3.8/site-packages/jax/_src/lib/xla_bridge.py:412: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if config.get('data_dir'):\n",
    "    logging.info('data_dir=%s', config.data_dir)\n",
    "logging.info('Output dir: %s', output_dir)\n",
    "\n",
    "save_checkpoint_path = None\n",
    "if config.get('checkpoint_steps'):\n",
    "    gfile.makedirs(output_dir)\n",
    "    save_checkpoint_path = os.path.join(output_dir, 'checkpoint.npz')\n",
    "    \n",
    "# Create an asynchronous multi-metric writer.\n",
    "writer = metric_writers.create_default_writer(\n",
    "    output_dir, just_logging=jax.process_index() > 0)\n",
    "\n",
    "# The pool is used to perform misc operations such as logging in async way.\n",
    "pool = multiprocessing.pool.ThreadPool()\n",
    "\n",
    "def write_note(note):\n",
    "    if jax.host_id() == 0:\n",
    "        logging.info('NOTE: %s', note)\n",
    "write_note('Initializing...')\n",
    "\n",
    "fillin = lambda *_: None\n",
    "# Verify settings to make sure no checkpoints are accidentally missed.\n",
    "if config.get('keep_checkpoint_steps'):\n",
    "    assert config.get('checkpoint_steps'), 'Specify `checkpoint_steps`.'\n",
    "    assert config.keep_checkpoint_steps % config.checkpoint_steps == 0, (\n",
    "        f'`keep_checkpoint_steps` ({config.checkpoint_steps}) should be'\n",
    "        f'divisible by `checkpoint_steps ({config.checkpoint_steps}).`')\n",
    "    \n",
    "batch_size = config.batch_size\n",
    "batch_size_eval = config.get('batch_size_eval', batch_size)\n",
    "if (batch_size % jax.device_count() != 0 or\n",
    "      batch_size_eval % jax.device_count() != 0):\n",
    "    raise ValueError(f'Batch sizes ({batch_size} and {batch_size_eval}) must '\n",
    "         f'be divisible by device number ({jax.device_count()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a223437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nband/ub_venv/lib/python3.8/site-packages/jax/_src/lib/xla_bridge.py:425: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Skipped importing the SMCalflow dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/datasets/datasets.py\", line 57, in <module>\n",
      "    from uncertainty_baselines.datasets.smcalflow import MultiWoZDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/datasets/smcalflow.py\", line 40, in <module>\n",
      "    import seqio\n",
      "ModuleNotFoundError: No module named 'seqio'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 452556574 3005505313]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipped importing the Speech Commands dataset due to OSError.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/datasets/datasets.py\", line 69, in <module>\n",
      "    from uncertainty_baselines.datasets.speech_commands import SpeechCommandsDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/datasets/speech_commands.py\", line 29, in <module>\n",
      "    import librosa\n",
      "  File \"/home/nband/ub_venv/lib/python3.8/site-packages/librosa/__init__.py\", line 211, in <module>\n",
      "    from . import core\n",
      "  File \"/home/nband/ub_venv/lib/python3.8/site-packages/librosa/core/__init__.py\", line 6, in <module>\n",
      "    from .audio import *  # pylint: disable=wildcard-import\n",
      "  File \"/home/nband/ub_venv/lib/python3.8/site-packages/librosa/core/audio.py\", line 8, in <module>\n",
      "    import soundfile as sf\n",
      "  File \"/home/nband/ub_venv/lib/python3.8/site-packages/soundfile.py\", line 142, in <module>\n",
      "    raise OSError('sndfile library not found')\n",
      "OSError: sndfile library not found\n",
      "WARNING:absl:Skipped importing the SMCalflow dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/datasets/__init__.py\", line 66, in <module>\n",
      "    from uncertainty_baselines.datasets.smcalflow import MultiWoZDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/datasets/smcalflow.py\", line 40, in <module>\n",
      "    import seqio\n",
      "ModuleNotFoundError: No module named 'seqio'\n",
      "WARNING:absl:Skipped importing the Speech Commands dataset due to OSError.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/datasets/__init__.py\", line 76, in <module>\n",
      "    from uncertainty_baselines.datasets.speech_commands import SpeechCommandsDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/datasets/speech_commands.py\", line 29, in <module>\n",
      "    import librosa\n",
      "  File \"/home/nband/ub_venv/lib/python3.8/site-packages/librosa/__init__.py\", line 211, in <module>\n",
      "    from . import core\n",
      "  File \"/home/nband/ub_venv/lib/python3.8/site-packages/librosa/core/__init__.py\", line 6, in <module>\n",
      "    from .audio import *  # pylint: disable=wildcard-import\n",
      "  File \"/home/nband/ub_venv/lib/python3.8/site-packages/librosa/core/audio.py\", line 8, in <module>\n",
      "    import soundfile as sf\n",
      "  File \"/home/nband/ub_venv/lib/python3.8/site-packages/soundfile.py\", line 142, in <module>\n",
      "    raise OSError('sndfile library not found')\n",
      "OSError: sndfile library not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UBDiabeticRetinopathyDetection builder config ub_diabetic_retinopathy_detection/btgraham-300.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Kaggle DR dataset with decision threshold: moderate.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f2a4ad0c100>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Removing feature ('name',) because dtype <dtype: 'string'> is not supported in JAX.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f2a4ad0c100>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f2a4ad0c100>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "{'image': ShardedDeviceArray([[[[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     ...,\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]]],\n",
      "\n",
      "\n",
      "\n",
      "                    [[[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     ...,\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]]],\n",
      "\n",
      "\n",
      "\n",
      "                    [[[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     ...,\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]]],\n",
      "\n",
      "\n",
      "\n",
      "                    ...,\n",
      "\n",
      "\n",
      "\n",
      "                    [[[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     ...,\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]]],\n",
      "\n",
      "\n",
      "\n",
      "                    [[[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     ...,\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]]],\n",
      "\n",
      "\n",
      "\n",
      "                    [[[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     ...,\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]],\n",
      "\n",
      "\n",
      "                     [[[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      ...,\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]],\n",
      "\n",
      "                      [[0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       ...,\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608],\n",
      "                       [0.5019608, 0.5019608, 0.5019608]]]]],                   dtype=float32), 'labels': ShardedDeviceArray([[[0., 1.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [0., 1.],\n",
      "                     [1., 0.],\n",
      "                     [0., 1.],\n",
      "                     [1., 0.]],\n",
      "\n",
      "                    [[1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [0., 1.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.]],\n",
      "\n",
      "                    [[1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [0., 1.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [0., 1.],\n",
      "                     [1., 0.]],\n",
      "\n",
      "                    [[1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.]],\n",
      "\n",
      "                    [[0., 1.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [0., 1.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.]],\n",
      "\n",
      "                    [[1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [0., 1.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.]],\n",
      "\n",
      "                    [[1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [0., 1.],\n",
      "                     [1., 0.]],\n",
      "\n",
      "                    [[1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.],\n",
      "                     [0., 1.],\n",
      "                     [1., 0.],\n",
      "                     [1., 0.]]], dtype=float32), 'mask': ShardedDeviceArray([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "                    [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "                    [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "                    [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "                    [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "                    [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "                    [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "                    [1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nband/ub_venv/lib/python3.8/site-packages/jax/_src/lib/xla_bridge.py:412: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "local_batch_size = batch_size // jax.host_count()\n",
    "local_batch_size_eval = batch_size_eval // jax.host_count()\n",
    "logging.info(\n",
    "  'Global batch size %d on %d hosts results in %d local batch size. '\n",
    "  'With %d devices per host (%d devices total), that\\'s a %d per-device '\n",
    "  'batch size.',\n",
    "  batch_size, jax.host_count(), local_batch_size,\n",
    "  jax.local_device_count(), jax.device_count(),\n",
    "  local_batch_size // jax.local_device_count())\n",
    "\n",
    "write_note('Initializing train dataset...')\n",
    "rng, train_ds_rng = jax.random.split(rng)\n",
    "train_ds_rng = jax.random.fold_in(train_ds_rng, jax.process_index())\n",
    "print(train_ds_rng)\n",
    "\n",
    "train_base_dataset = ub.datasets.get(\n",
    "    config.in_domain_dataset, split=config.train_split,\n",
    "    data_dir=config.get('data_dir'))\n",
    "train_dataset_builder = train_base_dataset._dataset_builder\n",
    "\n",
    "# Same for training and evaluation\n",
    "preproc_fn = preprocess_spec.parse(\n",
    "    spec=config.pp_train, available_ops=preprocess_utils.all_ops())\n",
    "\n",
    "train_ds = input_utils.get_data(\n",
    "    # dataset=config.in_domain_dataset,\n",
    "    dataset=train_dataset_builder,\n",
    "    split=config.train_split,\n",
    "    rng=train_ds_rng,\n",
    "    host_batch_size=local_batch_size,\n",
    "    preprocess_fn=preproc_fn,\n",
    "    shuffle_buffer_size=config.shuffle_buffer_size,\n",
    "    prefetch_size=config.get('prefetch_to_host', 2),\n",
    "    data_dir=config.get('data_dir'))\n",
    "\n",
    "# Start prefetching already.\n",
    "train_iter = input_utils.start_input_pipeline(\n",
    "    train_ds, config.get('prefetch_to_device', 1))\n",
    "\n",
    "print(next(train_iter))\n",
    "\n",
    "write_note('Initializing val dataset(s)...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3149cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(next(train_iter)['image'][0][0], interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d51175",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _get_val_split(dataset,\n",
    "                     split,\n",
    "                     pp_eval,\n",
    "                     data_dir=None):\n",
    "    del pp_eval  # Same as pp_train for Diabetic Retinopathy.\n",
    "\n",
    "    # We do ceil rounding such that we include the last incomplete batch.\n",
    "    nval_img = input_utils.get_num_examples(\n",
    "      dataset,\n",
    "      split=split,\n",
    "      host_batch_size=local_batch_size_eval,\n",
    "      drop_remainder=False,\n",
    "      data_dir=fillin(data_dir))\n",
    "    val_steps = int(np.ceil(nval_img / batch_size_eval))\n",
    "    logging.info('Running validation for %d steps for %s, %s', val_steps,\n",
    "                 dataset, split)\n",
    "\n",
    "    val_ds = input_utils.get_data(\n",
    "      # dataset=config.in_domain_dataset,\n",
    "      dataset=dataset,\n",
    "      split=config.val_split,\n",
    "      rng=None,\n",
    "      host_batch_size=local_batch_size_eval,\n",
    "      # preprocess_fn=preprocess_spec.parse(\n",
    "      #   spec=pp_eval, available_ops=preprocess_utils.all_ops()),\n",
    "      preprocess_fn=preproc_fn,\n",
    "      # cache=config.get('val_cache', 'batched'),\n",
    "      cache=False,\n",
    "      repeat_after_batching=True,\n",
    "      shuffle=False,\n",
    "      prefetch_size=config.get('prefetch_to_host', 2),\n",
    "      drop_remainder=False,\n",
    "      data_dir=config.get('data_dir'))\n",
    "    \n",
    "    val_iter = input_utils.start_input_pipeline(\n",
    "      val_ds, config.get('prefetch_to_device', 1))\n",
    "\n",
    "    return (val_iter, val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bdb97ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UBDiabeticRetinopathyDetection builder config ub_diabetic_retinopathy_detection/btgraham-300.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`drop_remainder` is deprecated. Please pass `remainder_options` instead. `remainder_options` is reset with RemainderOptions.BALANCE_ON_PROCESSES.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:Removing feature ('name',) because dtype <dtype: 'string'> is not supported in JAX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Kaggle DR dataset with decision threshold: moderate.\n"
     ]
    }
   ],
   "source": [
    "val_base_dataset = ub.datasets.get(\n",
    "    config.in_domain_dataset, split=config.val_split,\n",
    "    data_dir=config.get('data_dir'))\n",
    "val_dataset_builder = val_base_dataset._dataset_builder\n",
    "# val_preproc_fn = train_base_dataset._create_process_example_fn()\n",
    "val_iter_splits = {\n",
    "    'val': _get_val_split(\n",
    "        # config.in_domain_dataset,\n",
    "        val_dataset_builder,\n",
    "        config.val_split,\n",
    "        pp_eval=config.pp_eval,\n",
    "        data_dir=config.get('data_dir'))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbdb020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`drop_remainder` is deprecated. Please pass `remainder_options` instead. `remainder_options` is reset with RemainderOptions.DROP.\n",
      "WARNING:absl:Dropping 6 examples from the train split of the ub_diabetic_retinopathy_detection dataset.\n"
     ]
    }
   ],
   "source": [
    "ntrain_img = input_utils.get_num_examples(\n",
    "      # config.dataset,\n",
    "      # config.in_domain_dataset,\n",
    "      train_dataset_builder,\n",
    "      split=config.train_split,\n",
    "      host_batch_size=local_batch_size,\n",
    "      # data_dir=fillin(config.get('data_dir')))\n",
    "      data_dir=config.get('data_dir'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43637722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(next(val_iter_splits['val'][0])['image'][0][0], interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9873bd",
   "metadata": {},
   "outputs": [],
   "source": [
    " steps_per_epoch = ntrain_img / batch_size\n",
    "if config.get('num_epochs'):\n",
    "    total_steps = int(config.num_epochs * steps_per_epoch)\n",
    "    assert not config.get('total_steps'), 'Set either num_epochs or total_steps'\n",
    "else:\n",
    "    total_steps = config.total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4eddce6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nband/ub_venv/lib/python3.8/site-packages/jax/_src/lib/xla_bridge.py:412: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Skipped bert model import due to ImportError.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/models/models.py\", line 36, in <module>\n",
      "    from uncertainty_baselines.models.bert import bert_model  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/models/bert.py\", line 29, in <module>\n",
      "    from official.nlp import optimization\n",
      "ModuleNotFoundError: No module named 'official'\n",
      "WARNING:absl:Skipped BERT models due to ImportError.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/models/__init__.py\", line 86, in <module>\n",
      "    from uncertainty_baselines.models import bert\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/models/bert.py\", line 29, in <module>\n",
      "    from official.nlp import optimization\n",
      "ModuleNotFoundError: No module named 'official'\n",
      "WARNING:absl:Skipped Torch models due to ImportError.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/models/__init__.py\", line 112, in <module>\n",
      "    from uncertainty_baselines.models.resnet50_torch import resnet50_torch\n",
      "  File \"/home/nband/uncertainty-baselines/uncertainty_baselines/models/resnet50_torch.py\", line 24, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "logging.info(\n",
    "    'Running for %d steps, that means %f epochs and %f steps per epoch',\n",
    "    total_steps, total_steps * batch_size / ntrain_img, steps_per_epoch)\n",
    "\n",
    "write_note('Initializing model...')\n",
    "logging.info('config.model = %s', config.get('model'))\n",
    "model = ub.models.vision_transformer(\n",
    "    num_classes=config.num_classes, **config.get('model', {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b08e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want all parameters to be created in host RAM, not on any device, they'll\n",
    "# be sent there later as needed, otherwise we already encountered two\n",
    "# situations where we allocate them twice.\n",
    "@partial(jax.jit, backend='cpu')\n",
    "def init(rng):\n",
    "    # print(train_ds.element_spec['image'].shape)\n",
    "    image_size = tuple(train_ds.element_spec['image'].shape[2:])\n",
    "    # image_size = tuple(train_ds.element_spec['image'])\n",
    "#     image_size = (config.pp_input_res, config.pp_input_res, 3)\n",
    "    # print(image_size)\n",
    "    # image_size = tuple(train_ds.element_spec['features'].shape[2:])\n",
    "    logging.info('image_size = %s', image_size)\n",
    "    dummy_input = jnp.zeros((local_batch_size,) + image_size, jnp.float32)\n",
    "    params = flax.core.unfreeze(model.init(rng, dummy_input,\n",
    "                                           train=False))['params']\n",
    "\n",
    "    # Set bias in the head to a low value, such that loss is small initially.\n",
    "    params['head']['bias'] = jnp.full_like(\n",
    "      params['head']['bias'], config.get('init_head_bias', 0))\n",
    "\n",
    "    # init head kernel to all zeros for fine-tuning\n",
    "    if config.get('model_init'):\n",
    "        params['head']['kernel'] = jnp.full_like(params['head']['kernel'], 0)\n",
    "\n",
    "    return params\n",
    "\n",
    "rng, rng_init = jax.random.split(rng)\n",
    "params_cpu = init(rng_init)\n",
    "\n",
    "if jax.host_id() == 0:\n",
    "    num_params = sum(p.size for p in jax.tree_flatten(params_cpu)[0])\n",
    "    parameter_overview.log_parameter_overview(params_cpu)\n",
    "    writer.write_scalars(step=0, scalars={'num_params': num_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31796cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, axis_name='batch')\n",
    "def evaluation_fn(params, images, labels):\n",
    "    logits, out = model.apply({'params': flax.core.freeze(params)},\n",
    "                              images,\n",
    "                              train=False)\n",
    "    losses = getattr(train_utils, config.get('loss', 'softmax_xent'))(\n",
    "        logits=logits, labels=labels, reduction=False)\n",
    "    loss = jax.lax.psum(losses, axis_name='batch')\n",
    "    top1_idx = jnp.argmax(logits, axis=1)\n",
    "    # Extracts the label at the highest logit index for each image.\n",
    "    top1_correct = jnp.take_along_axis(labels, top1_idx[:, None], axis=1)[:, 0]\n",
    "    ncorrect = jax.lax.psum(top1_correct, axis_name='batch')\n",
    "    n = batch_size_eval\n",
    "    metric_args = jax.lax.all_gather([\n",
    "        logits, labels, out['pre_logits']],\n",
    "        axis_name='batch')\n",
    "    return ncorrect, loss, n, metric_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48d23b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimizer from flax.\n",
    "opt_name = config.get('optim_name')\n",
    "write_note(f'Initializing {opt_name} optimizer...')\n",
    "opt_def = getattr(flax.optim, opt_name)(**config.get('optim', {}))\n",
    "\n",
    "# We jit this, such that the arrays that are created are created on the same\n",
    "# device as the input is, in this case the CPU. Else they'd be on device[0].\n",
    "opt_cpu = jax.jit(opt_def.create)(params_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae24d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, axis_name='batch', donate_argnums=(0,))\n",
    "def update_fn(opt, lr, images, labels, rng):\n",
    "    \"\"\"Update step.\"\"\"\n",
    "\n",
    "    measurements = {}\n",
    "\n",
    "    # Get device-specific loss rng.\n",
    "    rng, rng_model = jax.random.split(rng, 2)\n",
    "    rng_model_local = jax.random.fold_in(rng_model, jax.lax.axis_index('batch'))\n",
    "\n",
    "    def loss_fn(params, images, labels):\n",
    "        logits, _ = model.apply(\n",
    "            {'params': flax.core.freeze(params)}, images,\n",
    "            train=True, rngs={'dropout': rng_model_local})\n",
    "        \n",
    "#         print('train logits')\n",
    "#         print(logits.primal)\n",
    "#         print('train labels')\n",
    "#         print(labels.primal)\n",
    "        loss = getattr(train_utils, config.get('loss', 'sigmoid_xent'))(\n",
    "            logits=logits, labels=labels)\n",
    "        return loss\n",
    "\n",
    "    # Implementation considerations compared and summarized at\n",
    "    # https://docs.google.com/document/d/1g3kMEvqu1DOawaflKNyUsIoQ4yIVEoyE5ZlIPkIl4Lc/edit?hl=en#\n",
    "    l, g = train_utils.accumulate_gradient(\n",
    "      jax.value_and_grad(loss_fn), opt.target, images, labels,\n",
    "      config.get('grad_accum_steps'))\n",
    "    l, g = jax.lax.pmean((l, g), axis_name='batch')\n",
    "    print(l, g)\n",
    "\n",
    "    # Log the gradient norm only if we need to compute it anyways (clipping)\n",
    "    # or if we don't use grad_accum_steps, as they interact badly.\n",
    "    if config.get('grad_accum_steps', 1) == 1 or config.get('grad_clip_norm'):\n",
    "        grads, _ = jax.tree_flatten(g)\n",
    "        l2_g = jnp.sqrt(sum([jnp.vdot(p, p) for p in grads]))\n",
    "        measurements['l2_grads'] = l2_g\n",
    "\n",
    "    # Optionally resize the global gradient to a maximum norm. We found this\n",
    "    # useful in some cases across optimizers, hence it's in the main loop.\n",
    "    if config.get('grad_clip_norm'):\n",
    "        g_factor = jnp.minimum(1.0, config.grad_clip_norm / l2_g)\n",
    "        g = jax.tree_util.tree_map(lambda p: g_factor * p, g)\n",
    "    opt = opt.apply_gradient(g, learning_rate=lr)\n",
    "\n",
    "    decay_rules = config.get('weight_decay', []) or []\n",
    "    if isinstance(decay_rules, numbers.Number):\n",
    "        decay_rules = [('.*kernel.*', decay_rules)]\n",
    "    sched_m = lr / config.lr.base if config.get('weight_decay_decouple') else lr\n",
    "\n",
    "    def decay_fn(v, wd):\n",
    "        return (1.0 - sched_m * wd) * v\n",
    "\n",
    "    opt = opt.replace(\n",
    "      target=train_utils.tree_map_with_regex(decay_fn, opt.target,\n",
    "                                             decay_rules))\n",
    "\n",
    "    params, _ = jax.tree_flatten(opt.target)\n",
    "    measurements['l2_params'] = jnp.sqrt(sum([jnp.vdot(p, p) for p in params]))\n",
    "\n",
    "    return opt, l, rng, measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6defb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "# train_batch = next(train_iter)\n",
    "\n",
    "# logits, _ = model.apply(\n",
    "#     {'params': flax.core.freeze(params)}, images,\n",
    "#     train=True, rngs={'dropout': rng_model_local})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38dbe096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other things besides optimizer state to be stored.\n",
    "rng, rng_loop = jax.random.split(rng, 2)\n",
    "rngs_loop = flax_utils.replicate(rng_loop)\n",
    "checkpoint_extra = dict(accum_train_time=0.0, rngs_loop=rngs_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70e3cdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1238482944 bytes == 0x1435c4000 @  0x7f2a54a54680 0x7f2a54a75824 0x7f2a1b186328 0x7f29733eb0a8 0x7f29733defc1 0x5f5e79 0x5f6a46 0x50b4a7 0x5703e6 0x5696da 0x5f6403 0x56b5e0 0x5f6226 0x56b3fe 0x5f6226 0x5703e6 0x5696da 0x68db17 0x600f34 0x5c4ad0 0x56b3fe 0x50053d 0x56cdfc 0x50053d 0x56cdfc 0x50053d 0x5042e6 0x56b5e0 0x5f6226 0x56b3fe 0x5f6226\n",
      "tcmalloc: large alloc 1238482944 bytes == 0x191b42000 @  0x7f2a54a54680 0x7f2a54a74ff4 0x7f2a5456a379 0x7f2a5456c009 0x7f2a5456c0a6 0x7f29733eb142 0x7f29733defc1 0x5f5e79 0x5f6a46 0x50b4a7 0x5703e6 0x5696da 0x5f6403 0x56b5e0 0x5f6226 0x56b3fe 0x5f6226 0x5703e6 0x5696da 0x68db17 0x600f34 0x5c4ad0 0x56b3fe 0x50053d 0x56cdfc 0x50053d 0x56cdfc 0x50053d 0x5042e6 0x56b5e0 0x5f6226\n"
     ]
    }
   ],
   "source": [
    "# Decide how to initialize training. The order is important.\n",
    "# 1. Always resumes from the existing checkpoint, e.g. resumes a finetune job.\n",
    "# 2. Resume from a previous checkpoint, e.g. start a cooldown training job.\n",
    "# 3. Initialize model from something, e,g, start a fine-tuning job.\n",
    "# 4. Train from scratch.\n",
    "resume_checkpoint_path = None\n",
    "if save_checkpoint_path and gfile.exists(save_checkpoint_path):\n",
    "    resume_checkpoint_path = save_checkpoint_path\n",
    "    \n",
    "# elif config.get('resume'):\n",
    "#     resume_checkpoint_path = fillin(config.resume)\n",
    "# if resume_checkpoint_path:\n",
    "#     write_note('Resume training from checkpoint...')\n",
    "#     checkpoint_tree = {'opt': opt_cpu, 'extra': checkpoint_extra}\n",
    "#     checkpoint = checkpoint_utils.load_checkpoint(checkpoint_tree,\n",
    "#                                                   resume_checkpoint_path)\n",
    "#     opt_cpu, checkpoint_extra = checkpoint['opt'], checkpoint['extra']\n",
    "#     rngs_loop = checkpoint_extra['rngs_loop']\n",
    "# elif config.get('model_init'):\n",
    "\n",
    "write_note(f'Initialize model from {config.model_init}...')\n",
    "reinit_params = config.get('model_reinit_params',\n",
    "                           ('head/kernel', 'head/bias'))\n",
    "logging.info('Reinitializing these parameters: %s', reinit_params)\n",
    "loaded = checkpoint_utils.load_from_pretrained_checkpoint(\n",
    "  params_cpu, config.model_init, config.model.representation_size,\n",
    "  config.model.classifier, reinit_params)\n",
    "opt_cpu = opt_cpu.replace(target=loaded)\n",
    "\n",
    "if jax.host_id() == 0:\n",
    "    logging.info('Restored parameter overview:')\n",
    "    parameter_overview.log_parameter_overview(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cf7d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_note('Kicking off misc stuff...')\n",
    "first_step = int(opt_cpu.state.step)  # Might be a DeviceArray type.\n",
    "if first_step == 0 and jax.host_id() == 0:\n",
    "    writer.write_hparams(dict(config))\n",
    "chrono = train_utils.Chrono(first_step, total_steps, batch_size,\n",
    "                          checkpoint_extra['accum_train_time'])\n",
    "# Note: switch to ProfileAllHosts() if you need to profile all hosts.\n",
    "# (Xprof data become much larger and take longer to load for analysis)\n",
    "profiler = periodic_actions.Profile(\n",
    "    # Create profile after every restart to analyze pre-emption related\n",
    "    # problems and assure we get similar performance in every run.\n",
    "    logdir=output_dir, first_profile=first_step + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bfa4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the learning-rate and pre-fetch it to device to avoid delays.\n",
    "lr_fn = train_utils.create_learning_rate_schedule(total_steps,\n",
    "                                                **config.get('lr', {}))\n",
    "# TODO(dusenberrymw): According to flax docs, prefetching shouldn't be\n",
    "# necessary for TPUs.\n",
    "lr_iter = train_utils.prefetch_scalar(\n",
    "    map(lr_fn, range(total_steps)), config.get('prefetch_to_device', 1))\n",
    "\n",
    "write_note(f'Replicating...\\n{chrono.note}')\n",
    "opt_repl = flax_utils.replicate(opt_cpu)\n",
    "\n",
    "checkpoint_writer = None\n",
    "\n",
    "# Note: we return the train loss, val loss, and fewshot best l2s for use in\n",
    "# reproducibility unit tests.\n",
    "train_loss = -jnp.inf\n",
    "val_loss = -jnp.inf\n",
    "results = {'dummy': {(0, 1): -jnp.inf}}\n",
    "\n",
    "write_note(f'First step compilations...\\n{chrono.note}')\n",
    "logging.info('first_step = %s', first_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c2e9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advance the iterators if we are restarting from an earlier checkpoint.\n",
    "# TODO(dusenberrymw): Look into checkpointing dataset state instead.\n",
    "if first_step > 0:\n",
    "    write_note('Advancing iterators after resuming from a checkpoint...')\n",
    "    lr_iter = itertools.islice(lr_iter, first_step, None)\n",
    "    train_iter = itertools.islice(train_iter, first_step, None)\n",
    "    # NOTE: Validation eval is only run on certain steps, so determine how many\n",
    "    # times it was run previously.\n",
    "    num_val_runs = sum(\n",
    "        map(\n",
    "            lambda i: train_utils.itstime(i, config.log_eval_steps,\n",
    "                                          total_steps\n",
    "                                          ), range(1, first_step + 1)))\n",
    "    for val_name, (val_iter, val_steps) in val_iter_splits.items():\n",
    "        val_iter = itertools.islice(val_iter, num_val_runs * val_steps,\n",
    "                                    None)\n",
    "        val_iter_splits[val_name] = (val_iter, val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d166f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=0/1)> {'Transformer': {'encoder_norm': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'encoderblock_0': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_1': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_10': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_11': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_2': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_3': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_4': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_5': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_6': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_7': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_8': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'encoderblock_9': {'LayerNorm_0': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'LayerNorm_2': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'scale': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>}, 'MlpBlock_3': {'Dense_0': {'bias': Traced<ShapedArray(float32[3072])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,3072])>with<DynamicJaxprTrace(level=0/1)>}, 'Dense_1': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[3072,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'MultiHeadDotProductAttention_1': {'key': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'out': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[12,64,768])>with<DynamicJaxprTrace(level=0/1)>}, 'query': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}, 'value': {'bias': Traced<ShapedArray(float32[12,64])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,12,64])>with<DynamicJaxprTrace(level=0/1)>}}}, 'posembed_input': {'pos_embedding': Traced<ShapedArray(float32[1,17,768])>with<DynamicJaxprTrace(level=0/1)>}}, 'cls': Traced<ShapedArray(float32[1,1,768])>with<DynamicJaxprTrace(level=0/1)>, 'embedding': {'bias': Traced<ShapedArray(float32[768])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[16,16,3,768])>with<DynamicJaxprTrace(level=0/1)>}, 'head': {'bias': Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/1)>, 'kernel': Traced<ShapedArray(float32[768,2])>with<DynamicJaxprTrace(level=0/1)>}}\n",
      "[0.6931472 0.6931472 0.6931472 0.6931472 0.6931472 0.6931472 0.6931472\n",
      " 0.6931472]\n",
      "[775.0109 775.0109 775.0109 775.0109 775.0109 775.0109 775.0109 775.0109]\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "DEBUG = True\n",
    "\n",
    "if DEBUG:\n",
    "    step, train_batch, lr_repl = first_step + 1, next(train_iter), next(lr_iter)\n",
    "    train_batch = next(train_iter)\n",
    "    opt_repl, loss_value, rngs_loop, extra_measurements = update_fn(\n",
    "        opt_repl,\n",
    "        lr_repl,\n",
    "        train_batch['image'],\n",
    "        train_batch['labels'],\n",
    "        rng=rngs_loop)\n",
    "\n",
    "    print(loss_value)\n",
    "    print(extra_measurements['l2_params'])\n",
    "\n",
    "    train_batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a python integer for step here, because opt.state.step is allocated\n",
    "# on TPU during replication.\n",
    "for step, train_batch, lr_repl in zip(\n",
    "    range(first_step + 1, total_steps + 1), train_iter, lr_iter):\n",
    "\n",
    "    with jax.profiler.TraceContext('train_step', step_num=step, _r=1):\n",
    "        opt_repl, loss_value, rngs_loop, extra_measurements = update_fn(\n",
    "            opt_repl,\n",
    "            lr_repl,\n",
    "            train_batch['image'],\n",
    "            train_batch['labels'],\n",
    "            rng=rngs_loop)\n",
    "\n",
    "    if jax.host_id() == 0:\n",
    "        profiler(step)\n",
    "\n",
    "    # Checkpoint saving\n",
    "    if train_utils.itstime(\n",
    "        step, config.get('checkpoint_steps'), total_steps, host=0):\n",
    "        write_note('Checkpointing...')\n",
    "        chrono.pause()\n",
    "        train_utils.checkpointing_timeout(checkpoint_writer,\n",
    "                                          config.get('checkpoint_timeout',\n",
    "                                                     1))\n",
    "        checkpoint_extra['accum_train_time'] = chrono.accum_train_time\n",
    "        checkpoint_extra['rngs_loop'] = rngs_loop\n",
    "        # We need to transfer the weights over now or else we risk keeping them\n",
    "        # alive while they'll be updated in a future step, creating hard to debug\n",
    "        # memory errors (see b/160593526). Also, takes device 0's params only.\n",
    "        opt_cpu = jax.tree_util.tree_map(lambda x: np.array(x[0]), opt_repl)\n",
    "\n",
    "        # Check whether we want to keep a copy of the current checkpoint.\n",
    "        copy_step = None\n",
    "        if train_utils.itstime(step, config.get('keep_checkpoint_steps'),\n",
    "                               total_steps):\n",
    "            write_note('Keeping a checkpoint copy...')\n",
    "            copy_step = step\n",
    "\n",
    "        # Checkpoint should be a nested dictionary or FLAX datataclasses from\n",
    "        # `flax.struct`. Both can be present in a checkpoint.\n",
    "        checkpoint = {'opt': opt_cpu, 'extra': checkpoint_extra}\n",
    "        checkpoint_writer = pool.apply_async(\n",
    "            checkpoint_utils.save_checkpoint,\n",
    "            (checkpoint, save_checkpoint_path, copy_step))\n",
    "        chrono.resume()\n",
    "\n",
    "    # Report training progress\n",
    "    if train_utils.itstime(\n",
    "        step, config.log_training_steps, total_steps, host=0):\n",
    "        write_note('Reporting training progress...')\n",
    "        train_loss = loss_value[\n",
    "            0]  # Keep to return for reproducibility tests.\n",
    "        timing_measurements, note = chrono.tick(step)\n",
    "        write_note(note)\n",
    "        train_measurements = {}\n",
    "        train_measurements.update({\n",
    "            'learning_rate': lr_repl[0],\n",
    "            'training_loss': train_loss,\n",
    "        })\n",
    "        train_measurements.update(\n",
    "            flax.jax_utils.unreplicate(extra_measurements))\n",
    "        train_measurements.update(timing_measurements)\n",
    "        writer.write_scalars(step, train_measurements)\n",
    "\n",
    "    # Report validation performance\n",
    "    if train_utils.itstime(step, config.log_eval_steps, total_steps):\n",
    "        write_note('Evaluating on the validation set...')\n",
    "        chrono.pause()\n",
    "        for val_name, (val_iter, val_steps) in val_iter_splits.items():\n",
    "            # Sets up evaluation metrics.\n",
    "            ece_num_bins = config.get('ece_num_bins', 15)\n",
    "            auc_num_bins = config.get('auc_num_bins', 1000)\n",
    "            ece = rm.metrics.ExpectedCalibrationError(num_bins=ece_num_bins)\n",
    "            calib_auc = rm.metrics.CalibrationAUC(\n",
    "                correct_pred_as_pos_label=False)\n",
    "            oc_auc_0_5 = rm.metrics.OracleCollaborativeAUC(\n",
    "                oracle_fraction=0.005,\n",
    "                num_bins=auc_num_bins)\n",
    "            oc_auc_1 = rm.metrics.OracleCollaborativeAUC(\n",
    "                oracle_fraction=0.01,\n",
    "                num_bins=auc_num_bins)\n",
    "            oc_auc_2 = rm.metrics.OracleCollaborativeAUC(\n",
    "                oracle_fraction=0.02,\n",
    "                num_bins=auc_num_bins)\n",
    "            oc_auc_5 = rm.metrics.OracleCollaborativeAUC(\n",
    "                oracle_fraction=0.05,\n",
    "                num_bins=auc_num_bins)\n",
    "            label_diversity = tf.keras.metrics.Mean()\n",
    "            sample_diversity = tf.keras.metrics.Mean()\n",
    "            ged = tf.keras.metrics.Mean()\n",
    "\n",
    "            # Runs evaluation loop.\n",
    "            ncorrect, loss, nseen = 0, 0, 0\n",
    "            for _, batch in zip(range(val_steps), val_iter):\n",
    "                batch_ncorrect, batch_losses, batch_n, batch_metric_args = evaluation_fn(\n",
    "                    opt_repl.target, batch['image'], batch['labels'])\n",
    "\n",
    "                # All results are a replicated array shaped as follows:\n",
    "                # (local_devices, per_device_batch_size, elem_shape...)\n",
    "                # with each local device's entry being identical as they got psum'd.\n",
    "                # So let's just take the first one to the host as numpy.\n",
    "\n",
    "                # from jft/deterministic.py\n",
    "                ncorrect += np.sum(np.array(batch_ncorrect[0]))\n",
    "                loss += np.sum(np.array(batch_losses[0]))\n",
    "                nseen += np.sum(np.array(batch_n[0]))\n",
    "\n",
    "                if config.get('loss', 'sigmoid_xent') != 'sigmoid_xent':\n",
    "                    # Here we parse batch_metric_args to compute uncertainty metrics.\n",
    "                    # (e.g., ECE or Calibration AUC).\n",
    "                    logits, labels, _ = batch_metric_args\n",
    "                    logits = np.array(logits[0])\n",
    "                    probs = jax.nn.softmax(logits)\n",
    "                    # From one-hot to integer labels, as required by ECE.\n",
    "                    int_labels = np.argmax(np.array(labels[0]), axis=-1)\n",
    "                    int_preds = np.argmax(logits, axis=-1)\n",
    "                    confidence = np.max(probs, axis=-1)\n",
    "\n",
    "                    for p, c, l, d, label in zip(probs, confidence,\n",
    "                                                    int_labels,\n",
    "                                                    int_preds,\n",
    "                                                    labels[0]):\n",
    "                        ece.add_batch(p, label=l)\n",
    "                        calib_auc.add_batch(d, label=l, confidence=c)\n",
    "                        # TODO(jereliu): Extend to support soft multi-class probabilities.\n",
    "                        oc_auc_0_5.add_batch(\n",
    "                            d, label=l, custom_binning_score=c)\n",
    "                        oc_auc_1.add_batch(\n",
    "                            d, label=l, custom_binning_score=c)\n",
    "                        oc_auc_2.add_batch(\n",
    "                            d, label=l, custom_binning_score=c)\n",
    "                        oc_auc_5.add_batch(\n",
    "                            d, label=l, custom_binning_score=c)\n",
    "\n",
    "            val_loss = loss / nseen  # Keep to return for reproducibility tests.\n",
    "            val_measurements = {\n",
    "                f'{val_name}_prec@1': ncorrect / nseen,\n",
    "                f'{val_name}_loss': val_loss,\n",
    "                f'{val_name}_ece': ece.result()['ece'],\n",
    "                f'{val_name}_calib_auc': calib_auc.result()[\n",
    "                    'calibration_auc'],\n",
    "                f'{val_name}_oc_auc_0.5%': oc_auc_0_5.result()[\n",
    "                    'collaborative_auc'],\n",
    "                f'{val_name}_oc_auc_1%': oc_auc_1.result()[\n",
    "                    'collaborative_auc'],\n",
    "                f'{val_name}_oc_auc_2%': oc_auc_2.result()[\n",
    "                    'collaborative_auc'],\n",
    "                f'{val_name}_oc_auc_5%': oc_auc_5.result()[\n",
    "                    'collaborative_auc'],\n",
    "            }\n",
    "            writer.write_scalars(step, val_measurements)\n",
    "            print('Step: ', step)\n",
    "            print(val_measurements)\n",
    "\n",
    "        chrono.resume()\n",
    "\n",
    "    # End of step.\n",
    "    if config.get('testing_failure_step'):\n",
    "        # Break early to simulate infra failures in test cases.\n",
    "        if config.testing_failure_step == step:\n",
    "            break\n",
    "\n",
    "write_note(f'Done!\\n{chrono.note}')\n",
    "pool.close()\n",
    "pool.join()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2524fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0003e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
